#!/usr/bin/env python3
"""
Filtra datos problem√°ticos (sueldos menores al m√≠nimo legal).
"""

import pandas as pd
import logging
from pathlib import Path

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

BASE_DIR = Path(__file__).resolve().parent.parent

# Sueldo m√≠nimo m√°s conservador para no eliminar datos leg√≠timos
SUELDO_MINIMO_CHILE = 200000  # pesos chilenos (m√°s conservador)

def filtrar_datos_problematicos(df):
    """Filtra datos con sueldos menores al m√≠nimo legal."""
    if df.empty:
        return df
    
    logger.info("üîç Filtrando datos problem√°ticos...")
    
    # Datos originales
    total_original = len(df)
    
    # Identificar datos problem√°ticos
    datos_problematicos = df[df['sueldo_bruto'] < SUELDO_MINIMO_CHILE].copy()
    datos_validos = df[df['sueldo_bruto'] >= SUELDO_MINIMO_CHILE].copy()
    
    logger.info(f"üìä An√°lisis de datos problem√°ticos:")
    logger.info(f"  Total original: {total_original:,} registros")
    logger.info(f"  Datos problem√°ticos: {len(datos_problematicos):,} registros")
    logger.info(f"  Datos v√°lidos: {len(datos_validos):,} registros")
    
    if len(datos_problematicos) > 0:
        logger.info(f"  Porcentaje problem√°tico: {len(datos_problematicos)/total_original*100:.1f}%")
        
        # An√°lisis por organismo
        logger.info("\nüìã Datos problem√°ticos por organismo:")
        org_problematicos = datos_problematicos['organismo'].value_counts()
        for org, count in org_problematicos.items():
            logger.info(f"  {org}: {count} registros")
        
        # An√°lisis por fuente
        logger.info("\nüìã Datos problem√°ticos por fuente:")
        fuente_problematicos = datos_problematicos['fuente'].value_counts()
        for fuente, count in fuente_problematicos.items():
            logger.info(f"  {fuente}: {count} registros")
        
        # Guardar datos problem√°ticos para revisi√≥n
        output_problematicos = BASE_DIR / 'data' / 'processed' / 'datos_problematicos.csv'
        datos_problematicos.to_csv(output_problematicos, index=False, encoding='utf-8')
        logger.info(f"üìÅ Datos problem√°ticos guardados en: {output_problematicos}")
    
    return datos_validos

def main():
    """Funci√≥n principal."""
    logger.info("üöÄ Iniciando filtrado de datos problem√°ticos")
    
    # Cargar datos categorizados
    input_file = BASE_DIR / 'data' / 'processed' / 'sueldos_categorizados.parquet'
    
    if not input_file.exists():
        # Fallback a CSV
        input_file = BASE_DIR / 'data' / 'processed' / 'sueldos_categorizados.csv'
    
    if not input_file.exists():
        logger.error(f"No se encontr√≥ el archivo: {input_file}")
        return
    
    logger.info(f"Cargando datos desde: {input_file}")
    
    if input_file.suffix == '.parquet':
        df = pd.read_parquet(input_file)
    else:
        df = pd.read_csv(input_file)
    
    logger.info(f"Datos cargados: {len(df)} registros")
    
    # Filtrar datos problem√°ticos
    df_filtrado = filtrar_datos_problematicos(df)
    
    # Guardar datos filtrados
    output_file_parquet = BASE_DIR / 'data' / 'processed' / 'sueldos_filtrados.parquet'
    output_file_csv = BASE_DIR / 'data' / 'processed' / 'sueldos_filtrados.csv'
    
    df_filtrado.to_parquet(output_file_parquet, index=False, compression='snappy')
    df_filtrado.to_csv(output_file_csv, index=False, encoding='utf-8')
    
    # Crear archivo peque√±o para Streamlit Cloud
    df_small = df_filtrado.sample(n=min(5000, len(df_filtrado)), random_state=42)
    output_file_small_parquet = BASE_DIR / 'data' / 'processed' / 'sueldos_filtrados_small.parquet'
    output_file_small_csv = BASE_DIR / 'data' / 'processed' / 'sueldos_filtrados_small.csv'
    
    df_small.to_parquet(output_file_small_parquet, index=False, compression='snappy')
    df_small.to_csv(output_file_small_csv, index=False, encoding='utf-8')
    
    logger.info(f"‚úÖ Datos filtrados guardados en:")
    logger.info(f"  Parquet: {output_file_parquet}")
    logger.info(f"  CSV: {output_file_csv}")
    logger.info(f"  Parquet peque√±o: {output_file_small_parquet}")
    logger.info(f"  CSV peque√±o: {output_file_small_csv}")
    
    # Estad√≠sticas finales
    logger.info(f"\nüìä Estad√≠sticas finales:")
    logger.info(f"  Registros v√°lidos: {len(df_filtrado):,}")
    logger.info(f"  Organismos √∫nicos: {df_filtrado['organismo'].nunique()}")
    logger.info(f"  Categor√≠as √∫nicas: {df_filtrado['categoria_organismo'].nunique()}")
    logger.info(f"  Sueldo m√≠nimo: ${df_filtrado['sueldo_bruto'].min():,.0f}")
    logger.info(f"  Sueldo m√°ximo: ${df_filtrado['sueldo_bruto'].max():,.0f}")
    logger.info(f"  Sueldo promedio: ${df_filtrado['sueldo_bruto'].mean():,.0f}")
    
    # Distribuci√≥n por categor√≠a
    logger.info(f"\nüìã Distribuci√≥n por categor√≠a:")
    for categoria, count in df_filtrado['categoria_organismo'].value_counts().items():
        logger.info(f"  {categoria}: {count} funcionarios")

if __name__ == '__main__':
    main()
